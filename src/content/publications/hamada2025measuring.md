---
number: 165
title: 'Measuring How LLMs Internalize Human Psychological Concepts: A preliminary
  analysis'
authors: Hamada, H. T., Fujisawa, I., Kawakita, G., & Yamada, Y.
year: 2025
venue: arXiv
doi: https://doi.org/10.48550/arXiv.2506.23055
url: https://arxiv.org/abs/2506.23055
type: Paper
language: en
scholar: true
abstract: Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities
  in producing human-like text. However, it is unclear how accurately these models
  internalize concepts that shape human thought and behavior. Here, we developed a
  quantitative framework to assess concept alignment between LLMs and human psychological
  dimensions using 43 standardized psychological questionnaires, selected for their
  established validity in measuring distinct psychological constructs. Our method
  evaluates how accurately language models reconstruct and classify questionnaire
  items through pairwise similarity analysis. We compared resulting cluster structures
  with the original categorical labels using hierarchical clustering. A GPT-4 model
  achieved superior classification accuracy (66.2\%), significantly outperforming
  GPT-3.5 (55.9\%) and BERT (48.1\%), all exceeding random baseline performance (31.9\%).
  We also demonstrated that the estimated semantic similarity from GPT-4 is associated
  with Pearson's correlation coefficients of human responses in multiple psychological
  questionnaires. This framework provides a novel approach to evaluate the alignment
  of the human-LLM concept and identify potential representational biases. Our findings
  demonstrate that modern LLMs can approximate human psychological constructs with
  measurable accuracy, offering insights for developing more interpretable AI systems.
---

