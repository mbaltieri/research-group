---
number: 127
title: Delineating neural contributions to electroencephalogram-based speech decoding
authors: Sato, M., Kabe, Y., Nobe, S., Yoshida, A., Inoue, M., Shimizu, M., Tomeoka.,
  K., & Sasai, S.
year: 2024
venue: bioRxiv
doi: https://doi.org/10.1101/2024.05.09.591996
url: https://www.biorxiv.org/content/10.1101/2024.05.09.591996v1
type: Paper
language: en
scholar: true
abstract: Speech Brain-computer interfaces (BCIs) have emerged as a pivotal technology
  in facilitating communication for individuals with speech impairments. Utilizing
  electroencephalography (EEG) for noninvasive speech BCIs offers an accessible and
  affordable solution, potentially benefiting a broader audience. However, EEG-based
  speech decoding remains controversial especially for overt speech, due to difficulties
  in separating speech-related neural activities from myoelectric potential artifacts
  generated during articulation. Here we aim to delineate the extent of the neural
  contributions by employing Explainable AI techniques to a convolutional neural network
  predicting spoken words based on signals obtained by ultra-high-density (uhd)-EEG.
  We found that electrode-wise contributions to the decoding cannot be explained by
  their mutual information with electromyography (EMG). Furthermore, contributing
  periods of speech to EEG-based decoding are distinct from those to decoding solely
  relying on EMG. In contrast, there are significant overlaps in signal timings contributing
  to EEG-based decoding, regardless of vocal conditions such as overt or covert speech.
  Notably, the denoising process successfully enhanced the decoding contribution from
  electrodes within speech-related brain areas for all speech conditions. Altogether,
  our findings support the idea that, with appropriate preprocessing, EEG becomes
  a valuable tool for decoding spoken words based on underlying neural activities.
---

